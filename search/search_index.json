{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83d\udc4b Welcome Home","text":"<p>Welcome to Wordcab, your go-to destination for thought-provoking, informative, and entertaining blog posts!</p> <p>Whether you're looking to stay up-to-date on the latest industry trends or simply seeking a fresh perspective on everyday issues, we've got you covered.</p> <p>At Wordcab, we believe that knowledge is power, and we're dedicated to empowering our readers with engaging and insightful content that will enrich their lives. </p> <p>So why not take a few minutes to explore our site and discover something new today? With new posts added regularly, there's always something fresh and exciting to read. </p> <p>So go ahead, dive in, and join the conversation! \ud83d\udcac</p>"},{"location":"blog/2023/03/30/keep-your-workstation-clean-docker/","title":"Keep your workstation clean - Docker","text":""},{"location":"blog/2023/03/30/keep-your-workstation-clean-docker/#optimize-docker-storage-for-machine-learning","title":"Optimize Docker Storage for Machine Learning","text":"<p>When working with Machine Learning, especially with large images like NVIDIA ones for training models on GPUs, it is important to manage your workstation storage efficiently.</p> <p>Docker is a great tool for containerization, providing a consistent environment for deploying applications.</p> <p>However, as you create and run containers, unused files and storage may accumulate on your system.</p> <p>In this post, we'll cover how to use Docker commands to prevent unused files and storage from cluttering your workstation.</p>"},{"location":"blog/2023/03/30/keep-your-workstation-clean-docker/#1-remove-unused-containers","title":"1. Remove unused containers","text":"<p>Stopped containers can take up valuable storage space on your workstation. To remove them, use the following command:</p> <p> docker container prune </p> <p>This command will prompt you to confirm the deletion of stopped containers. Enter <code>y</code> to proceed. Be careful, as this action is irreversible.</p>"},{"location":"blog/2023/03/30/keep-your-workstation-clean-docker/#2-remove-unused-images","title":"2. Remove unused images","text":"<p>Unused images can also occupy significant storage space. I always have multiple NVIDIA images on my workstation because of the different versions of CUDA and cuDNN that I use for different projects.</p> <p>To remove unused images, use the following command:</p> <p> docker image prune </p> <p>Bye-bye, unused <code>cuda11.0-cudnn8-devel-ubuntu18.04</code> image! \ud83d\ude22</p> <p>But hey, now we are all using CUDA 12.1 and PyTorch 2.0, right? </p> <p>Right? \ud83d\ude05</p>"},{"location":"blog/2023/03/30/keep-your-workstation-clean-docker/#3-remove-unused-networks-and-volumes","title":"3. Remove unused networks and volumes","text":"<p>Unused networks and volumes can also take up storage space on your workstation.</p> <p>It's less the case when working with ML training, but if you have built some APIs on top of your ML models, you may have some unused networks and volumes. It's always a good idea to clean them up from time to time.</p> <p>To remove unused networks and volumes, use the following commands:</p> <p> docker network prune docker volume prune </p> <p>These commands will prompt you to confirm the deletion. Enter <code>y</code> to proceed.  Remember that this action is irreversible, so only run this command if you're sure you no longer need these resources.</p>"},{"location":"blog/2023/03/30/keep-your-workstation-clean-docker/#4-remove-the-build-cache","title":"4. Remove the build cache","text":"<p>One of the most common causes of unused files and storage is the build cache. I discovered how much space the build cache can take up when I was working on a project that required me to build a lot of Docker images.</p> <p>Thankfully, Docker provides a command to remove the build cache. To do so, use the following command:</p> <p> docker builder prune </p> <p>The build cache is used to accelerate the Docker image building process, but most of the time you don't need to keep it when you are going to build a new image for a new project.</p> <p>I saved 25GB of storage space on my workstation by removing the build cache of only one project. \ud83d\ude31</p> <p>Imagine how much space you can save if you remove the build cache after months of working on different projects!</p> <p>By using these Docker commands, you can efficiently manage your workstation's storage, preventing unused files from accumulating and optimizing space for machine learning tasks.</p> <p>Always be cautious when running these commands, as they will delete unused containers, images, networks, volumes, and build cache. Only run them if you are certain you no longer need the resources. With a clean workstation, you'll be better equipped to train your machine learning models using large images and Docker.</p>"},{"location":"blog/2023/04/04/turbocharge-your-tokenization-exploiting-parallelism/","title":"Turbocharge your tokenization exploiting parallelism","text":""},{"location":"blog/2023/04/04/turbocharge-your-tokenization-exploiting-parallelism/#parallelize-hugging-face-tokenizers-with-num_proc","title":"Parallelize Hugging Face Tokenizers with num_proc","text":"<p>Processing large datasets can be time-consuming, especially when it comes to tokenizing text.</p> <p>But what if you could reduce your tokenization time from hours to mere minutes? Without any extra effort? \ud83e\udd2f </p> <p>In this blog post, we'll show you how to parallelize your tokenization using Hugging Face's <code>num_proc</code> parameter.</p>"},{"location":"blog/2023/04/04/turbocharge-your-tokenization-exploiting-parallelism/#hugging-face-tokenizers","title":"Hugging Face Tokenizers","text":"<p>Hugging Face, \"the AI community building the future\", offer a library called <code>transformers</code>, which provides state-of-the-art models and tokenizers for various ML tasks. </p> <p>Tip</p> <p>If you're not familiar with Hugging Face, we highly recommend you check it out! You should be a cave-dwelling hermit not to have heard of it by now. \ud83e\udd37\u200d\u2642\ufe0f</p> <p>Tokenization is the process of converting text into smaller units (=tokens), such as words or subwords, that can be more easily processed by NLP models.</p> <p>It's an essential step in preparing your data for tasks like text classification, sentiment analysis, and machine translation.</p>"},{"location":"blog/2023/04/04/turbocharge-your-tokenization-exploiting-parallelism/#the-power-of-parallelization","title":"The power of parallelization","text":"<p>Parallelization is the technique of dividing a computational task into smaller subtasks that can be executed concurrently by multiple processors or cores.</p> <p>By breaking down the workload and distributing it among several processing units, parallelization can significantly speed up the execution of tasks, including tokenization.</p> <p>Thanks to its Rust implementation, Hugging Face's tokenizers are already blazingly fast. But what if you could make them even faster? \ud83d\ude80</p>"},{"location":"blog/2023/04/04/turbocharge-your-tokenization-exploiting-parallelism/#using-num_proc-for-parallelization","title":"Using <code>num_proc</code> for parallelization","text":"<p>Hugging Face tokenizers support a num_proc parameter that allows you to parallelize the tokenization process.</p> <p>By specifying the number of processes to run concurrently, you can take full advantage of your available processing power.  Nowadays, most computers have multiple cores, so you can easily speed up your tokenization by using all of them.</p> <p>Note</p> <p>You don't have to run on a beefy server to benefit from parallelization. Even a laptop with 4-8 cores can benefit from parallelization.</p> <p>Here's a code snippet illustrating how to use the num_proc parameter:</p> <pre><code>import multiprocessing\n\nfrom datasets import load_dataset\n\nfrom transformers import AutoTokenizer\n\n\n# Load any hugging face dataset\ndata = load_dataset(\"imdb\")\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n# Find your available cores\nnum_cores = multiprocessing.cpu_count()\n\n# Preprocess your data\ntrain_data = data[\"train\"].shuffle().map(lambda sample: tokenizer(sample[\"text\"]), num_proc=num_cores)\ntest_data = data[\"test\"].map(lambda sample: tokenizer(sample[\"text\"]), num_proc=num_cores)\n</code></pre> <p>If you don't want to use all your available cores, you can specify a lower number or even limit the number of cores to a specific range.</p> <pre><code># Use only 8 cores maximum\nnum_cores = min(8, multiprocessing.cpu_count())\n\n# Use all cores except the last one\nnum_cores = multiprocessing.cpu_count() - 1\n</code></pre> <p>Tip</p> <p>The lambda function is used to apply the tokenizer to each sample in the dataset. You can define your own function and pass it to the map method instead. It's totally dependent on your use case and the task you're trying to accomplish.</p>"},{"location":"blog/2023/04/04/turbocharge-your-tokenization-exploiting-parallelism/#time-to-parallelize","title":"Time to parallelize!","text":"<p>It could seem like a small change, but parallelization can make a huge difference in your tokenization time.</p> <p>For example, I had to process a 40GB dataset including more than 6 million samples. It take about 4 hours to tokenize it with a single core, but less than 5 minutes with 96 cores!</p> <p>I know, you're probably thinking \"that's great, but I don't have 96 cores\" \ud83d\ude05. You're right, but you can still benefit from parallelization, because most computers have at least 8-16 cores nowadays. And spending 20-30 minutes to tokenize a dataset is still a huge improvement compared to 4 hours.</p> <p>Parallelizing tokenization tasks with Hugging Face's <code>num_proc</code> parameter can yield impressive results, drastically reducing the time it takes to process large datasets.</p> <p>By taking advantage of parallelization, you can accelerate your natural language processing tasks and focus on what matters most: analyzing and interpreting your data. </p> <p>Don't forget to share captivating images of parallel processing in action! \ud83e\udd13 Tweet your parallelization at @chainyo_ai and let me know how much time you saved!</p>"},{"location":"blog/2023/03/29/we-are-all-learners/","title":"We are all learners","text":"<p>In today's fast-paced world of technology, learning and adapting to new developments is essential.  This is especially true in the field of Artificial Intelligence (AI), which is advancing at an unprecedented rate.</p> <p>As a learner, it can be overwhelming to keep up with the constant stream of new information and updates...</p> <p>One way to stay on top of this is by writing blog posts and news articles about your learning journey. In this essay, we will discuss the importance of writing about your learning in tech, particularly in AI, and how keeping a learning journal can be helpful.</p>"},{"location":"blog/2023/03/29/we-are-all-learners/#why-write-about-your-learning","title":"Why write about your learning?","text":"<p>Firstly, writing about your learning experiences helps to solidify your knowledge.</p> <p>When you put pen to paper (fingers to keyboard if you aren't a moldu), you are forced to articulate your thoughts and ideas.</p> <p>This process helps to clarify your understanding of a subject, identify any gaps in your knowledge, and highlight areas where you need to focus more attention. In addition, by writing about a topic, you are more likely to remember it in the long-term.</p>"},{"location":"blog/2023/03/29/we-are-all-learners/#how-many-medium-posts-saved-your-life","title":"How many Medium posts saved your life?","text":"<p>It's also true with Stack Overflow's posts!</p> <p>Writing about your learning experiences can help others.  When you share your knowledge with the wider community, you are contributing to the collective knowledge base.  Your blog posts and articles can help others who are also learning about AI or other tech-related topics.  This can be especially helpful for those who are just starting out and may not know where to begin.</p> <p>We are all learners, and we all have something to share. Keep in mind that everyday, someone had decided to start coding, or learning more about AI, or even just learning how to use a new tool. So, if you have learned something new, share it with others. You never know who you might help.</p>"},{"location":"blog/2023/03/29/we-are-all-learners/#keep-a-track","title":"Keep a track","text":"<p>Keeping a learning journal can be a valuable tool for self-reflection. By recording your thoughts and feelings about your learning journey, you can gain a better understanding of your own learning style, strengths, and weaknesses. This can help you to tailor your learning approach to suit your individual needs and preferences.</p> <p>Your co-workers and friends can also benefit from it! They can also provide you with valuable feedback and insights that can help you to improve your learning process.</p>"},{"location":"blog/2023/03/29/we-are-all-learners/#motivation-isnt-the-key","title":"Motivation, isn't the key?","text":"<p>Finally, writing about your learning experiences can be a source of motivation and accountability. When you publicly commit to learning about a topic and share your progress with others, you are more likely to stay motivated and focused. This is because you have created a sense of accountability and a desire to prove yourself to others.</p> <p>Writing is like hacking your brain to be more productive. It's a way to make your brain work for you, instead of against you.</p> <p>In conclusion, writing about your learning experiences in tech, particularly in AI, can be a valuable tool for solidifying your knowledge, sharing your knowledge with others, self-reflection, and motivation.</p> <p>You can also be that person who add value to the open-source community, and help others to learn more about AI.</p>"},{"location":"blog/2023/03/31/wordcab-transcribe/","title":"Wordcab Transcribe - An open-source ASR solution using Whisper, Docker and FastAPI","text":"<p>Automatic Speech Recognition (ASR) has become an essential tool for developers and businesses.  With Wordcab Transcribe, you can leverage ASR in your projects without relying on expensive third-party platforms.</p> <p>We've implemented an open-source ASR solution using Docker, FastAPI, and the faster-whisper library, which is a fast  implementation of the transcription model from OpenAI Whisper. </p> <p>This project utilizes CTranslate2 under the hood to speed up the processing of audio files while requiring less  than 5GB of VRAM on the GPU with the large-v2 Whisper model.</p> <p>In this blog post, we'll present the Wordcab Transcribe project and show you how to use it in your own applications.</p>"},{"location":"blog/2023/03/31/wordcab-transcribe/#why-wordcab-transcribe","title":"Why Wordcab Transcribe?","text":"<p>Wordcab Transcribe offers several advantages over closed ASR platforms:</p> <ul> <li>\ud83e\udd17 Open-source: Our project is open-source and based on open-source libraries, allowing you to customize and extend it as needed.</li> <li>\u26a1 Fast: The faster-whisper library and CTranslate2 make audio processing incredibly fast compared to other implementations.</li> <li>\ud83d\udc33 Easy to deploy: You can deploy the project on your workstation or in the cloud using Docker.</li> <li>\ud83d\udd25 Batch requests: You can transcribe multiple audio files at once because batch requests are implemented in the API.</li> <li>\ud83d\udcb8 Cost-effective: As an open-source solution, you won't have to pay for costly ASR platforms.</li> <li>\ud83e\udef6 Easy-to-use API: With just a few lines of code, you can use the API to transcribe audio files or even YouTube videos.</li> </ul> <p>All the code is available on GitHub.</p> <p>Note</p> <p>The project is still in its early stages, so it may not be suitable for production use.  However, we are working on improving it and adding new features.</p> <p>If you have any suggestions or feedback, please let us know in the GitHub issues.</p>"},{"location":"blog/2023/03/31/wordcab-transcribe/#building-and-running-the-project","title":"Building and running the project","text":"<p>We use Docker to allow you to deploy the project on your workstation or in the cloud. It also avoids the hassle of installing all the dependencies on your workstation and matching the versions of the CUDA and cuDNN libraries.</p>"},{"location":"blog/2023/03/31/wordcab-transcribe/#prerequisites","title":"Prerequisites","text":"<p>To build and run the project, there are some prerequisites:</p> <ul> <li>Docker: You'll need to install Docker on your workstation. You can follow the instructions here.</li> <li>NVIDIA GPU: You'll need an NVIDIA GPU with at least 5GB of VRAM to run the project.</li> </ul> <p>Tip</p> <p>The project uses the <code>large-v2</code> model from OpenAI Whisper, which requires at least 2.5GB of VRAM, and with a batch size of 1, it's another 2GB of VRAM. So, you'll need at least 5GB of VRAM to run the project.</p> <p>The project was tested on a workstation with an NVIDIA RTX 3090 GPU and 24GB of VRAM and it worked fine with a batch size of 4.</p> <ul> <li>NVIDIA Container Toolkit: You'll need to install the NVIDIA Container Toolkit on your workstation. You can follow the instructions here.</li> </ul>"},{"location":"blog/2023/03/31/wordcab-transcribe/#building-the-docker-image","title":"Building the Docker image","text":"<p>To build the Docker image, run the following command:</p> <p> docker build -t wordcab-transcribe:latest . </p> <p>Depending on your internet connection, it may take a few minutes to download the NVIDIA base image.</p>"},{"location":"blog/2023/03/31/wordcab-transcribe/#running-the-docker-container","title":"Running the Docker container","text":"<p>To run the Docker container, run the following command:</p> <p> docker run -d --name wordcab-transcribe \\     --gpus all \\     --shm-size 1g \\     --restart unless-stopped \\     -p 5001:5001 \\     wordcab-transcribe:latest </p> <ul> <li><code>--gpus all</code> - Use all the GPUs on the workstation. Example using a specific GPU: <code>--gpus \"device=1\"</code>.</li> <li><code>--shm-size 1g</code> - Increase the shared memory size to 1GB. This allow PyTorch to use more memory, which can be useful when using large models and multiple GPUs.</li> </ul> <p>The container will start and you'll be able to use the API once the models are downloaded and loaded.</p>"},{"location":"blog/2023/03/31/wordcab-transcribe/#updating-configuration","title":"Updating configuration","text":"<p>You can update the configuration by editing the <code>.env</code> file.</p> <p>The configuration file is used to set the following parameters:</p> <pre><code>PROJECT_NAME=\"Wordcab Transcribe\"\nVERSION=\"0.1.0\"\nDESCRIPTION=\"ASR FastAPI server using faster-whisper and pyannote-audio.\"\nAPI_PREFIX=\"/api/v1\"\nDEBUG=True\nBATCH_SIZE=4\nMAX_WAIT=0.1\nWHISPER_MODEL=\"large-v2\"\nEMBEDDINGS_MODEL=\"speechbrain/spkrec-ecapa-voxceleb\"\nCOMPUTE_TYPE=\"int8_float16\"\n</code></pre> <p>To customize your deployment, you can edit the <code>.env</code> file and rebuild the Docker image.</p> <ul> <li>BATCH_SIZE - The maximum number of audio files to process in parallel. The default value is 4.</li> <li>MAX_WAIT - The maximum number of seconds to wait to process the audio files in the queue.  The default value is 0.1, which means that after 100ms, the audio files in the queue will be processed even if the  batch size is not reached.</li> <li>WHISPER_MODEL - The name of the Whisper model to use. The default value is <code>large-v2</code>.</li> <li>EMBEDDINGS_MODEL - The name of the speech embeddings model to use. The default value is <code>speechbrain/spkrec-ecapa-voxceleb</code>.</li> <li>COMPUTE_TYPE - The compute type to use. The default value is <code>int8_float16</code>.</li> </ul>"},{"location":"blog/2023/03/31/wordcab-transcribe/#api-endpoints","title":"API Endpoints","text":"<p>We provide two endpoints to transcribe audio:</p> <ol> <li><code>/api/v1/audio</code> - Accepts an audio file as input.</li> <li><code>/api/v1/youtube</code> - Accepts a YouTube video URL as input.</li> </ol>"},{"location":"blog/2023/03/31/wordcab-transcribe/#transcribing-an-audio-file","title":"Transcribing an audio file","text":"<p>To transcribe an audio file, use the <code>/api/v1/audio</code> endpoint. Here's an example using Python and the <code>requests</code> library:</p> <pre><code>import requests\n\nfilepath = \"sample_1.mp3\"\nurl = \"http://localhost:5001/api/v1/audio\"\n\nwith open(filepath, \"rb\") as f:\n    files = {\"file\": (filepath, f)}\n    response = requests.post(url, files=files)\n\nprint(response.json())\n</code></pre> <p>Alternatively, you can use <code>curl</code> in the terminal:</p> <p> curl -X 'POST' \\     'http://localhost:5001/api/v1/youtube' \\     -H 'accept: application/json' \\     -H 'Content-Type: multipart/form-data' \\     -F 'file=@/path/to/audio/file.wav' </p>"},{"location":"blog/2023/03/31/wordcab-transcribe/#transcribing-a-youtube-video","title":"Transcribing a YouTube video","text":"<p>To transcribe a YouTube video, use the <code>/api/v1/youtube</code> endpoint. Here's an example using Python and the <code>requests</code> library:</p> <pre><code>import requests\n\nvideo_url = \"https://youtu.be/dQw4w9WgXcQ\"\nurl = f\"http://localhost:5001/api/v1/youtube?url={video_url}\"\nresponse = requests.post(url)\n\nprint(response.json())\n</code></pre> <p>Or with <code>curl</code>:</p> <p> curl -X 'POST' \\     'http://localhost:5001/api/v1/youtube?url=https://youtu.be/dQw4w9WgXcQ' </p> <p>Wordcab Transcribe is a powerful, open-source ASR solution built with Docker, FastAPI, and the faster-whisper library.</p> <p>By implementing this project in your applications, you can leverage the benefits of ASR without the costs and restrictions of commercial platforms.</p> <p>With its easy-to-use API, blazing-fast performance, and low VRAM usage, Wordcab Transcribe is an excellent  choice for developers and businesses seeking a reliable, cost-effective ASR solution. </p> <p>Give it a try, and let the open-source community help you unlock the potential of ASR in your projects.</p>"}]}