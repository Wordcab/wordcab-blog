{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83d\udc4b Welcome Home","text":"<p>Welcome to Wordcab, your go-to destination for thought-provoking, informative, and entertaining blog posts!</p> <p>Whether you're looking to stay up-to-date on the latest industry trends or simply seeking a fresh perspective on everyday issues, we've got you covered.</p> <p>At Wordcab, we believe that knowledge is power, and we're dedicated to empowering our readers with engaging and insightful content that will enrich their lives. </p> <p>So why not take a few minutes to explore our site and discover something new today? With new posts added regularly, there's always something fresh and exciting to read. </p> <p>So go ahead, dive in, and join the conversation! \ud83d\udcac</p>"},{"location":"blog/2023/03/30/keep-your-workstation-clean-docker/","title":"Keep your workstation clean - Docker","text":""},{"location":"blog/2023/03/30/keep-your-workstation-clean-docker/#optimize-docker-storage-for-machine-learning","title":"Optimize Docker Storage for Machine Learning","text":"<p>When working with Machine Learning, especially with large images like NVIDIA ones for training models on GPUs, it is important to manage your workstation storage efficiently.</p> <p>Docker is a great tool for containerization, providing a consistent environment for deploying applications.</p> <p>However, as you create and run containers, unused files and storage may accumulate on your system.</p> <p>In this post, we'll cover how to use Docker commands to prevent unused files and storage from cluttering your workstation.</p>"},{"location":"blog/2023/03/30/keep-your-workstation-clean-docker/#1-remove-unused-containers","title":"1. Remove unused containers","text":"<p>Stopped containers can take up valuable storage space on your workstation. To remove them, use the following command:</p> <p> docker container prune </p> <p>This command will prompt you to confirm the deletion of stopped containers. Enter <code>y</code> to proceed. Be careful, as this action is irreversible.</p>"},{"location":"blog/2023/03/30/keep-your-workstation-clean-docker/#2-remove-unused-images","title":"2. Remove unused images","text":"<p>Unused images can also occupy significant storage space. I always have multiple NVIDIA images on my workstation because of the different versions of CUDA and cuDNN that I use for different projects.</p> <p>To remove unused images, use the following command:</p> <p> docker image prune </p> <p>Bye-bye, unused <code>cuda11.0-cudnn8-devel-ubuntu18.04</code> image! \ud83d\ude22</p> <p>But hey, now we are all using CUDA 12.1 and PyTorch 2.0, right? </p> <p>Right? \ud83d\ude05</p>"},{"location":"blog/2023/03/30/keep-your-workstation-clean-docker/#3-remove-unused-networks-and-volumes","title":"3. Remove unused networks and volumes","text":"<p>Unused networks and volumes can also take up storage space on your workstation.</p> <p>It's less the case when working with ML training, but if you have built some APIs on top of your ML models, you may have some unused networks and volumes. It's always a good idea to clean them up from time to time.</p> <p>To remove unused networks and volumes, use the following commands:</p> <p> docker network prune docker volume prune </p> <p>These commands will prompt you to confirm the deletion. Enter <code>y</code> to proceed.  Remember that this action is irreversible, so only run this command if you're sure you no longer need these resources.</p>"},{"location":"blog/2023/03/30/keep-your-workstation-clean-docker/#4-remove-the-build-cache","title":"4. Remove the build cache","text":"<p>One of the most common causes of unused files and storage is the build cache. I discovered how much space the build cache can take up when I was working on a project that required me to build a lot of Docker images.</p> <p>Thankfully, Docker provides a command to remove the build cache. To do so, use the following command:</p> <p> docker builder prune </p> <p>The build cache is used to accelerate the Docker image building process, but most of the time you don't need to keep it when you are going to build a new image for a new project.</p> <p>I saved 25GB of storage space on my workstation by removing the build cache of only one project. \ud83d\ude31</p> <p>Imagine how much space you can save if you remove the build cache after months of working on different projects!</p> <p>By using these Docker commands, you can efficiently manage your workstation's storage, preventing unused files from accumulating and optimizing space for machine learning tasks.</p> <p>Always be cautious when running these commands, as they will delete unused containers, images, networks, volumes, and build cache. Only run them if you are certain you no longer need the resources. With a clean workstation, you'll be better equipped to train your machine learning models using large images and Docker.</p>"},{"location":"blog/2023/03/29/we-are-all-learners/","title":"We are all learners","text":"<p>In today's fast-paced world of technology, learning and adapting to new developments is essential.  This is especially true in the field of Artificial Intelligence (AI), which is advancing at an unprecedented rate.</p> <p>As a learner, it can be overwhelming to keep up with the constant stream of new information and updates...</p> <p>One way to stay on top of this is by writing blog posts and news articles about your learning journey. In this essay, we will discuss the importance of writing about your learning in tech, particularly in AI, and how keeping a learning journal can be helpful.</p>"},{"location":"blog/2023/03/29/we-are-all-learners/#why-write-about-your-learning","title":"Why write about your learning?","text":"<p>Firstly, writing about your learning experiences helps to solidify your knowledge.</p> <p>When you put pen to paper (fingers to keyboard if you aren't a moldu), you are forced to articulate your thoughts and ideas.</p> <p>This process helps to clarify your understanding of a subject, identify any gaps in your knowledge, and highlight areas where you need to focus more attention. In addition, by writing about a topic, you are more likely to remember it in the long-term.</p>"},{"location":"blog/2023/03/29/we-are-all-learners/#how-many-medium-posts-saved-your-life","title":"How many Medium posts saved your life?","text":"<p>It's also true with Stack Overflow's posts!</p> <p>Writing about your learning experiences can help others.  When you share your knowledge with the wider community, you are contributing to the collective knowledge base.  Your blog posts and articles can help others who are also learning about AI or other tech-related topics.  This can be especially helpful for those who are just starting out and may not know where to begin.</p> <p>We are all learners, and we all have something to share. Keep in mind that everyday, someone had decided to start coding, or learning more about AI, or even just learning how to use a new tool. So, if you have learned something new, share it with others. You never know who you might help.</p>"},{"location":"blog/2023/03/29/we-are-all-learners/#keep-a-track","title":"Keep a track","text":"<p>Keeping a learning journal can be a valuable tool for self-reflection. By recording your thoughts and feelings about your learning journey, you can gain a better understanding of your own learning style, strengths, and weaknesses. This can help you to tailor your learning approach to suit your individual needs and preferences.</p> <p>Your co-workers and friends can also benefit from it! They can also provide you with valuable feedback and insights that can help you to improve your learning process.</p>"},{"location":"blog/2023/03/29/we-are-all-learners/#motivation-isnt-the-key","title":"Motivation, isn't the key?","text":"<p>Finally, writing about your learning experiences can be a source of motivation and accountability. When you publicly commit to learning about a topic and share your progress with others, you are more likely to stay motivated and focused. This is because you have created a sense of accountability and a desire to prove yourself to others.</p> <p>Writing is like hacking your brain to be more productive. It's a way to make your brain work for you, instead of against you.</p> <p>In conclusion, writing about your learning experiences in tech, particularly in AI, can be a valuable tool for solidifying your knowledge, sharing your knowledge with others, self-reflection, and motivation.</p> <p>You can also be that person who add value to the open-source community, and help others to learn more about AI.</p>"},{"location":"blog/2023/03/31/wordcab-transcribe/","title":"Wordcab Transcribe: An open-source ASR solution using Whisper, Docker and FastAPI","text":"<p>Automatic Speech Recognition (ASR) has become an essential tool for developers and businesses.  With Wordcab Transcribe, you can leverage ASR in your projects without relying on expensive third-party platforms.</p> <p>We've implemented an open-source ASR solution using Docker, FastAPI, and the faster-whisper library, which is a fast  implementation of the transcription model from OpenAI Whisper. </p> <p>This project utilizes CTranslate2 under the hood to speed up the processing of audio files while requiring less  than 5GB of VRAM on the GPU with the large-v2 Whisper model.</p> <p>In this blog post, we'll present the Wordcab Transcribe project and show you how to use it in your own applications.</p>"},{"location":"blog/2023/03/31/wordcab-transcribe/#why-wordcab-transcribe","title":"Why Wordcab Transcribe?","text":"<p>Wordcab Transcribe offers several advantages over closed ASR platforms:</p> <ul> <li>Open-source: Our project is open-source and based on open-source libraries, allowing you to customize and extend it as needed.</li> <li>Fast: The faster-whisper library and CTranslate2 make audio processing incredibly fast compared to other implementations.</li> <li>Easy to deploy: You can deploy the project on your workstation or in the cloud using Docker.</li> <li>Batch requests: You can transcribe multiple audio files at once because batch requests are implemented in the API.</li> <li>Cost-effective: As an open-source solution, you won't have to pay for costly ASR platforms.</li> <li>Easy-to-use API: With just a few lines of code, you can use the API to transcribe audio files or even YouTube videos.</li> </ul> <p>All the code is available on GitHub.</p>"},{"location":"blog/2023/03/31/wordcab-transcribe/#building-and-running-the-project","title":"Building and running the project","text":"<p>We use Docker to allow you to deploy the project on your workstation or in the cloud. It also avoids the hassle of installing all the dependencies on your workstation and matching the versions of the CUDA and cuDNN libraries.</p>"},{"location":"blog/2023/03/31/wordcab-transcribe/#prerequisites","title":"Prerequisites","text":"<p>To build and run the project, there are some prerequisites:</p> <ul> <li>Docker: You'll need to install Docker on your workstation. You can follow the instructions here.</li> <li>NVIDIA GPU: You'll need an NVIDIA GPU with at least 5GB of VRAM to run the project.</li> </ul> <p>Note</p> <p>The project uses the <code>large-v2</code> model from OpenAI Whisper, which requires at least 2.5GB of VRAM, and with a batch size of 1, it's another 2GB of VRAM. So, you'll need at least 5GB of VRAM to run the project.</p> <p>The project was tested on a workstation with an NVIDIA RTX 3090 GPU and 24GB of VRAM and it worked fine with a batch size of 4.</p> <ul> <li>NVIDIA Container Toolkit: You'll need to install the NVIDIA Container Toolkit on your workstation. You can follow the instructions here.</li> </ul>"},{"location":"blog/2023/03/31/wordcab-transcribe/#building-the-docker-image","title":"Building the Docker image","text":"<p>To build the Docker image, run the following command:</p> <p> docker build -t wordcab-transcribe:latest . </p> <p>Depending on your internet connection, it may take a few minutes to download the NVIDIA base image.</p>"},{"location":"blog/2023/03/31/wordcab-transcribe/#running-the-docker-container","title":"Running the Docker container","text":"<p>To run the Docker container, run the following command:</p> <p> docker run -d --name wordcab-transcribe \\     --gpus all \\     --ipc=host \\     --shm-size 64g \\     --ulimit memlock=1 \\     --ulimit stack=67108864 \\     -p 5001:5001 \\     --restart unless-stopped \\     wordcab-transcribe:latest </p> <ul> <li><code>--gpus all</code> - Use all the GPUs on the workstation. Example using a specific GPU: <code>--gpus \"device=1\"</code>.</li> <li><code>--shm-size 64g</code> - Increase the shared memory size to 64GB. This allow PyTorch to use more memory, which can be useful when using large models and multiple GPUs.</li> <li><code>--ipc=host</code> - Share the host's IPC namespace. This allows the container to benefit from the host's shared memory.</li> <li><code>--ulimit memlock=1</code> - Allow the container to lock memory on the GPU. This is recommended by NVIDIA for GPU containers.</li> <li><code>--ulimit stack=67108864</code> - Set the maximum stack size to 64MB. This can be useful for preventing stack overflow errors that may occur if the container tries to allocate more stack memory than is available.</li> </ul> <p>The container will start and you'll be able to use the API once the models are downloaded and loaded.</p>"},{"location":"blog/2023/03/31/wordcab-transcribe/#api-endpoints","title":"API Endpoints","text":"<p>We provide two endpoints to transcribe audio:</p> <ol> <li><code>/api/v1/audio</code> - Accepts an audio file as input.</li> <li><code>/api/v1/youtube</code> - Accepts a YouTube video URL as input.</li> </ol>"},{"location":"blog/2023/03/31/wordcab-transcribe/#transcribing-an-audio-file","title":"Transcribing an audio file","text":"<p>To transcribe an audio file, use the <code>/api/v1/audio</code> endpoint. Here's an example using Python and the <code>requests</code> library:</p> <pre><code>import requests\n\nfilepath = \"sample_1.mp3\"\nurl = \"http://localhost:5001/api/v1/audio\"\n\nwith open(filepath, \"rb\") as f:\n    files = {\"file\": (filepath, f)}\n    response = requests.post(url, files=files)\n\nprint(response.json())\n</code></pre> <p>Alternatively, you can use <code>curl</code> in the terminal:</p> <p> curl -X 'POST' \\   'http://localhost:5001/api/v1/youtube' \\   -H 'accept: application/json' \\   -H 'Content-Type: multipart/form-data' \\   -F 'file=@/path/to/audio/file.wav' </p>"},{"location":"blog/2023/03/31/wordcab-transcribe/#transcribing-a-youtube-video","title":"Transcribing a YouTube video","text":"<p>To transcribe a YouTube video, use the <code>/api/v1/youtube</code> endpoint. Here's an example using Python and the <code>requests</code> library:</p> <pre><code>import requests\n\nvideo_url = \"https://youtu.be/dQw4w9WgXcQ\"\nurl = f\"http://localhost:5001/api/v1/youtube?url={video_url}\"\nresponse = requests.post(url)\n\nprint(response.json())\n</code></pre> <p>Or with <code>curl</code>:</p> <p> curl -X POST \"http://localhost:5001/api/v1/youtube?url=https://youtu.be/dQw4w9WgXcQ\" </p> <p>Wordcab Transcribe is a powerful, open-source ASR solution built with Docker, FastAPI, and the faster-whisper library.</p> <p>By implementing this project in your applications, you can leverage the benefits of ASR without the costs and restrictions of commercial platforms.</p> <p>With its easy-to-use API, blazing-fast performance, and low VRAM usage, Wordcab Transcribe is an excellent  choice for developers and businesses seeking a reliable, cost-effective ASR solution. </p> <p>Give it a try, and let the open-source community help you unlock the potential of ASR in your projects.</p>"}]}